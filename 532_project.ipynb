{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qOh9FkR90pKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367b79ad-f7d4-45b6-b1c9-eeefc58fdfe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqwkUDmz8NrC"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Preprocessing credit card transaction data\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CreditCardFraudDetection\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load dataset\n",
        "file_path = '/content/drive/Shared drives/532_project/creditcard.csv'\n",
        "data = spark.read.csv(file_path, header=True, inferSchema=True)## Need to mount drive\n",
        "\n",
        "# data cleaning\n",
        "data = data.dropna()  # Drop missing values\n",
        "\n",
        "# data = data.withColumn(\"Time_scaled\", col(\"Time\") / 3600)  # Scale time to days\n",
        "# # Get all column names\n",
        "# columns_to_select = [f\"V{i}\" for i in range(1, 29)] + [\"Time\", \"Amount\", \"Class\"]\n",
        "# features = data.select(*columns_to_select)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# features.show()\n",
        "data.show(100)\n",
        "input_cols = [f\"V{i}\" for i in range(1, 29)] + [\"Time\", \"Amount\"]\n"
      ],
      "metadata": {
        "id": "M-4JDW9WpSwU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Prepare data for training\n",
        "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
        "processed_data = assembler.transform(data).select(\"features\", \"Class\")\n",
        "\n",
        "# Split data\n",
        "train, test = processed_data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", numTrees=100)\n",
        "model = rf.fit(train)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "22yNsUGOo1qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "# Evaluate model\n",
        "predictions = model.transform(test)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Class\", metricName=\"areaUnderPR\")\n",
        "auprc = evaluator.evaluate(predictions)\n",
        "print(f\"AUPRC: {auprc}\")\n",
        "\n",
        "# 2. Precision, Recall, and F1-Score\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"Class\", metricName=\"weightedPrecision\")\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"Class\", metricName=\"weightedRecall\")\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"Class\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1_score}\")"
      ],
      "metadata": {
        "id": "eb8nb6qOo2-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bin/zookeeper-server-start.sh config/zookeeper.properties\n",
        "# bin/kafka-server-start.sh config/server.properties\n",
        "# bin/kafka-topics.sh --create --topic creditcard-transactions --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1\n",
        "\n",
        "\"\"\" Commands to set Kafka server up  \"\"\"\n",
        "# Install kafka locally\n",
        "# Run in terminal to set the server locally"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BSh3BaetGlIK",
        "outputId": "d76b7824-81a7-4273-bff2-d2d8a9188b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Commands to set Kafka server up  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kafka-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jQhRv-nGQXZ",
        "outputId": "bc775a79-69e4-43a1-a30e-f464a4d8d7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl.metadata (7.8 kB)\n",
            "Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/246.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eBvd3mPJ-4m",
        "outputId": "112cb7b3-5dc7-4f7b-8aea-f5e09bc5ac00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Kafka Integration\n",
        "from kafka import KafkaProducer\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/Shared drives/532_project/creditcard.csv' # For now the dataset\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "Kafka producer setup\n",
        "producer = KafkaProducer(bootstrap_servers='0.0.0.0:9092',\n",
        "                         value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
        "\n",
        "producer = KafkaProducer(bootstrap_servers='8.tcp.ngrok.io:15664',\n",
        "                         value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
        "\n",
        "\n",
        "producer = KafkaProducer(\n",
        "    bootstrap_servers='0.tcp.ngrok.io:16640',\n",
        "    value_serializer=lambda x: json.dumps(x).encode('utf-8'),\n",
        "    retries=5,\n",
        "    request_timeout_ms=120000  # Timeout after 2 minutes\n",
        "    # metadata_fetch_timeout_ms=60000  # Timeout for metadata fetch after 1 minute\n",
        ")\n",
        "# producer = KafkaProducer(\n",
        "#     bootstrap_servers=['4.tcp.ngrok.io:19393:9092'],\n",
        "#     request_timeout_ms=30000,\n",
        "#     metadata_max_age_ms=60000\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Send data row by row\n",
        "# for index, row in data.iterrows():\n",
        "#     producer.send('creditcard-transactions', value=row.to_dict())\n",
        "#     time.sleep(0.01)  # Simulate delay between transactions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "zU1MdsPT9ohq",
        "outputId": "4489140c-7d1d-4423-9da8-07517efac203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NoBrokersAvailable",
          "evalue": "NoBrokersAvailable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-0e75403765f4>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#     # metadata_fetch_timeout_ms=60000  # Timeout for metadata fetch after 1 minute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m producer = KafkaProducer(\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mbootstrap_servers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'6.tcp.ngrok.io:18406:9092'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mrequest_timeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/producer/kafka.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         client = KafkaClient(metrics=self._metrics, metric_group_prefix='producer',\n\u001b[0m\u001b[1;32m    382\u001b[0m                              \u001b[0mwakeup_timeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_block_ms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                              **self.config)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/client_async.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mcheck_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version_auto_timeout_ms'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_can_bootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mcheck_version\u001b[0;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoBrokersAvailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType\n",
        "\n",
        "# Define transaction schema\n",
        "schema = StructType([\n",
        "    StructField(\"Time\", DoubleType(), True),\n",
        "    StructField(\"V1\", DoubleType(), True),\n",
        "    # Include all fields up to V28\n",
        "    StructField(\"Amount\", DoubleType(), True),\n",
        "    StructField(\"Class\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"FraudDetectionStreaming\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read from Kafka\n",
        "kafka_stream = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "    .option(\"subscribe\", \"creditcard-transactions\") \\\n",
        "    .load()\n",
        "\n",
        "# Parse the Kafka stream\n",
        "parsed_stream = kafka_stream.selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
        "    .select(\"data.*\")\n",
        "\n",
        "# Process data\n",
        "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
        "processed_stream = assembler.transform(parsed_stream)\n",
        "\n",
        "# Apply the trained model\n",
        "predictions = model.transform(processed_stream)\n",
        "\n",
        "# Write flagged fraud alerts to another Kafka topic\n",
        "predictions.selectExpr(\"to_json(struct(*)) AS value\") \\\n",
        "    .writeStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "    .option(\"topic\", \"fraud-alerts\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n"
      ],
      "metadata": {
        "id": "stpMNxJbEIxY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}